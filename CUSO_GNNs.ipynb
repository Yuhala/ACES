{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuhala/ACES/blob/master/CUSO_GNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A gentle introduction to Graph Neural Networks\n",
        "\n",
        "This session will introduce you to the basic concepts of graph neural networks. We will be using the popular deep learning framework pytorch and the graph specific PyG framework. Let's start with installing all the dependencies that we will need. Throughout this tutorial, we will use [pyG](https://https://pytorch-geometric.readthedocs.io/en/latest/), a graph learning framework built on top of pytorch."
      ],
      "metadata": {
        "id": "up-0Y2FIdYW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install torchmetrics\n",
        "\n",
        "import networkx\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn.conv import MessagePassing"
      ],
      "metadata": {
        "id": "lYLVSQVHdohA",
        "outputId": "bbd61aa3-7c28-455d-af3a-e89e2bd03439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.1 torchmetrics-1.3.0.post0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also provide a function that can be used to visualize graphs later."
      ],
      "metadata": {
        "id": "Hrd_-EdHmgix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils.convert import to_networkx, from_networkx\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from networkx.drawing.nx_pydot import graphviz_layout\n",
        "import torch_geometric.utils as utils\n",
        "\n",
        "# draw a pyG graph\n",
        "def visualize_datapt(data):\n",
        "    X = data.x\n",
        "    Y = data.y\n",
        "    g = to_networkx(data)\n",
        "    gl_x = {i:(i) for i,x in enumerate(X)}\n",
        "    layout = nx.spring_layout(g)\n",
        "    pY, pX = None, None\n",
        "    inp = len(X[0])\n",
        "    if inp == 1:\n",
        "      pX = [(x.item(),x.item(),1-abs(x.item())) for x in X]\n",
        "      pY = [(x.item(),1-abs(x.item()),x.item()) for x in Y]\n",
        "    elif inp == 2:\n",
        "      pX = [(x[0].item(),x[0].item(),1-abs(x[0].item())) for x in X]\n",
        "      pY = [(x.item(),1-abs(x.item()),x.item()) for x in Y]\n",
        "    plt.subplot(121)\n",
        "    nx.draw(g, pos=layout, with_labels = True, node_color=pX, labels=gl_x)\n",
        "    plt.subplot(122)\n",
        "    nx.draw(g, pos=layout, with_labels = True, node_color=pY)\n",
        "    plt.show()\n",
        "\n",
        "# Doesn't assume labels\n",
        "def visualize_graph(data):\n",
        "    G = utils.to_networkx(data)\n",
        "    pos = nx.spring_layout(G)\n",
        "    nx.draw(G, pos, with_labels=True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "izb4qHeQiFUT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BLOCK 1: Pytorch Geometric Basics\n"
      ],
      "metadata": {
        "id": "36OykZ-YiO1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Graph Representation in Pytorch\n",
        "\n",
        "Before we can start training a model, we need to think about how our graphs will be represented. While an adjacency matrix would be a valid option, it grows quadratically in the size of nodes, which consumes more memory than necessary when dealing with sparse graphs. This is why we use a tensor of shape (2, #edges) (A tensor can be understood as a multidimensional array) that contains pairs of node id's that represent (directed) edges (also known as COO format). For undirected edges we can simply add both directed edges."
      ],
      "metadata": {
        "id": "PCLTB0DciT_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The edge_index fully defines the graph topology\n",
        "edge_index = torch.tensor([[0, 1, 1, 2, 3 , 4, 5, 5],\n",
        "                           [1, 0, 2, 1, 2, 3, 4, 1]], dtype=torch.long)"
      ],
      "metadata": {
        "id": "LpsA5Wkcoq2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When learning with graphs, we typically endow nodes with some information, also referred to as features. This can be done with a tensor of shape (#nodes, #features)."
      ],
      "metadata": {
        "id": "0DEh_wK6orfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[0.5], [0], [1], [0], [1], [0.1]], dtype=torch.float)"
      ],
      "metadata": {
        "id": "CrhpGJptpYPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will deal with one specific instance of Machine Learning: supervised learning. The basic workflow is as follows: We are given a training dataset of graphs and their node features, as well as ground truth labels, for example for nodes. We then use this training dataset to fit a model, teaching it to map graphs to the right predictions. Once we are done with the training, we test the performance of the model on fresh data that it has not seen previously.\n",
        "To define the ground-truth data, we define a new tensor of size #nodes."
      ],
      "metadata": {
        "id": "kZQUipcmpfLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we have one (binary) class label per node\n",
        "y = torch.tensor([[0], [1], [1], [0], [1], [1]], dtype=torch.float)"
      ],
      "metadata": {
        "id": "secg70C3qnEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have all the necessary data, we can combine the edge_index, node features and ground-truth labels into one data object"
      ],
      "metadata": {
        "id": "utr8zccPqn16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch_graph = Data(x=x, edge_index=edge_index, y=y)\n"
      ],
      "metadata": {
        "id": "LfKGtr01qzo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the resulting data point."
      ],
      "metadata": {
        "id": "v8w-30U1q3nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Torch graph object:\")\n",
        "print(torch_graph)\n",
        "print(\"Node features\")\n",
        "print(torch_graph.x)\n",
        "print(\"Edge index\")\n",
        "print(torch_graph.edge_index)\n",
        "\n",
        "visualize_datapt(torch_graph)"
      ],
      "metadata": {
        "id": "z0f1eCMpiL8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the left, nodes are colored regarding their features. The right graph shows the ground-truth labels in different colors."
      ],
      "metadata": {
        "id": "f1lZD4zm5wir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graphs from networkx\n",
        "PyG makes it easy to convert graphs from frameworks such as networkx into its own format. This can simplify the generation of new dataopints significantly."
      ],
      "metadata": {
        "id": "GmG0ZZMDid5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = nx.grid_graph([2,4,3])\n",
        "nx.draw(g)\n",
        "\n",
        "# All we need is one function call\n",
        "networkx_graph = from_networkx(g)\n",
        "print(networkx_graph)"
      ],
      "metadata": {
        "id": "Fm-lhPKXieW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use our newly acquired knowledge to create complete datasets now. In this first example, we create random trees in networkx and label leafs for the ground-truth. This means that the task we want to solve is identifying whether a given node is a leaf or not. As nodes do not have features in this task, we simply provide all nodes with the same initial feature."
      ],
      "metadata": {
        "id": "_npLqvRaijVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trees():\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def gen_graph(self, num_nodes, num):\n",
        "        nx_graph = nx.random_tree(n=num_nodes, seed=num)\n",
        "        tree = from_networkx(nx_graph)\n",
        "        leafs = [x for x in nx_graph.nodes() if nx_graph.degree(x)==1]\n",
        "        tree.x = torch.ones(num_nodes, 1)\n",
        "        tree.y = torch.zeros(num_nodes)\n",
        "        for node in leafs:\n",
        "            tree.y[node] = 1\n",
        "        return tree\n",
        "\n",
        "    def makedata(self, num_graphs = 200, num_nodes = 8):\n",
        "        return [self.gen_graph(num_nodes, i) for i in range(num_graphs)]\n",
        "\n",
        "\n",
        "data = Trees().makedata(num_graphs = 20, num_nodes = 8)"
      ],
      "metadata": {
        "id": "JJTvTCgHigyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset is just a list of data objects\n",
        "visualize_datapt(data[0])"
      ],
      "metadata": {
        "id": "1mmCFa0hr5rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batching in pyG\n",
        "When training a machine-learning model, we usually want to train with multiple datapoints/graphs at the same, also because we can make use of parallel computations on our GPU. pyG has a unique approach to dealing with multiple graphs simultaneously, which we will look at now. Let's generate two simple graphs first."
      ],
      "metadata": {
        "id": "SCikir2kxofe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph1 = Data(x=torch.tensor([[1.0], [2.0], [3.0]]),\n",
        "              edge_index=torch.tensor([[0, 1, 1, 2],\n",
        "                                      [1, 0, 2, 1]]))\n",
        "graph2 = Data(x=torch.tensor([[10.0], [11.0], [12.0], [13.0]]),\n",
        "              edge_index=torch.tensor([[0, 1, 1, 2, 2, 3, 3, 0],\n",
        "                                      [1, 0, 2, 1, 3, 2, 0, 3]]))"
      ],
      "metadata": {
        "id": "iKlUY7KFxs8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_graph(graph1)"
      ],
      "metadata": {
        "id": "N1SgfG5UyNM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_graph(graph2)"
      ],
      "metadata": {
        "id": "sol0hHySyqgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we actually want to do when batching graphs is to concatenate all tensors (like `x` for labels, the `edge_index` and possibly also predictions `y`) such that the resulting object looks like a normal data object, but actually contains multiple disconnected components that represent our graphs. pyG offers a simple interface for this:"
      ],
      "metadata": {
        "id": "v1EnDu5ay4bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Batch\n",
        "batch = Batch.from_data_list([graph1, graph2])"
      ],
      "metadata": {
        "id": "uIKl7ZkH0EeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_graph(batch)"
      ],
      "metadata": {
        "id": "dHsBL8Kz0Ktx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at how the batch was created more closely."
      ],
      "metadata": {
        "id": "lVXyNXuL0Sb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch)\n",
        "print(batch.x)\n",
        "print(batch.edge_index)"
      ],
      "metadata": {
        "id": "10nFvkMl0XYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels `x` were simply concatenated, while the `edge_index` requires special attention. To keep the nodes of the two graphs in separate components, the id's of nodes in the second graph are shifted by the number of nodes in the first graph.\n",
        "In addition to `x` and `edge_index`, the data object now also contains two new attributes, `batch` and `ptr`. While `ptr` doesn't have to concern us too much for now, `batch` is important, as it allows us to do the inverse mapping from a batch to its individual graphs (\"debatching\")."
      ],
      "metadata": {
        "id": "OrcUtUBn1ioL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch.batch"
      ],
      "metadata": {
        "id": "L06LTXxx2erj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To facilitate this, the tensor keeps track of the indices of the graph that every node belongs to. In this case, they are `0` and `1`. This information is important when we want to apply some operations seperately on every graph in a batch, like global pooling (more on that later)."
      ],
      "metadata": {
        "id": "I-19LoqT0aAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##First GNN\n",
        "Let's implement our first Graph Neural Network using a graph convolution called GIN ([paper](https://arxiv.org/pdf/1810.00826.pdf)). The update rule for GIN is defined as:\n",
        "\n",
        "$\\displaystyle x_v^{(t+1)} = \\Theta((1+\\epsilon)\\cdot x_v^{(t)} + \\sum_{u \\in N(v)} x_u^{(t)})$\n",
        "\n",
        "where $x_v^{(t)}$ is the embedding of node $v$ at iteration $t$. In every iteration we execute the convolution synchronously for all nodes. For every node $v$, it aggreagates all neighboring states by summing them up and combining them with the old state of $v$ (using some learnable parameter $\\epsilon$). The result is then passed through the MLP $\\Theta$.\n",
        "Let's see how to use this convolutional layer."
      ],
      "metadata": {
        "id": "a6AXV4L6ioDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# The GIN convolution itself is already implemented and ready to use\n",
        "from torch_geometric.nn import GINConv\n",
        "\n",
        "# A GNN looks very similar to any other neural network that we implement in pytorch\n",
        "class GIN(torch.nn.Module):\n",
        "    # The constructor creates all neural networks that we need to run the model, but doesn't run them yet.\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # In this example, we want to run two separate GIN convolutions with different parameters\n",
        "        # We first create the MLPs \\Theta for the two convolutions\n",
        "        # The first convolution takes the one-dimensional input labels and maps them to 4-dimensional embeddings\n",
        "        self.network1 = torch.nn.Sequential(torch.nn.Linear(1, 4), torch.nn.ReLU())\n",
        "        # These 4-dimensional embeddings are used as input for the second convoluion, which maps them to outputs for the two classes we want to predict\n",
        "        self.network2 = torch.nn.Sequential(torch.nn.Linear(4, 2), torch.nn.ReLU())\n",
        "\n",
        "        # Finally, we can use the networks to construct the two convolutions\n",
        "        self.conv1 = GINConv(nn = self.network1)\n",
        "        self.conv2 = GINConv(nn = self.network2)\n",
        "\n",
        "    # The forward pass uses the networks that were defined in the constructor and evaluates them on given data\n",
        "    def forward(self, data):\n",
        "        # From the data, we get the node features x and the edge index\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # We pass x and edge_index to the convolutions\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        # Here we apply a log_softmax to get class probablities for the two outputs\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "qxg4bE0filnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our first GNN, let's use it on a dataset! The Tree dataset contains trees with leaf nodes marked as 1 and non-leaf nodes as 0 in the ground-truth. But first, we need some boilerplate code for our training loop. There is nothing unexpected here, we just run 20 epochs of training."
      ],
      "metadata": {
        "id": "Vb4NsPhNiwPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, dataset):\n",
        "  criterion = nn.NLLLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "  val_split = 0.8\n",
        "\n",
        "  np.random.shuffle(dataset)\n",
        "  train_size = int(val_split*len(dataset))\n",
        "  train_loader = DataLoader(dataset[:train_size], batch_size=1, shuffle=True)\n",
        "  model.train()\n",
        "  device = torch.device('cpu')\n",
        "  for epoch in range(20):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "          data = data.to(device)\n",
        "          pred = model(data)\n",
        "\n",
        "          loss = criterion(pred, data.y.to(torch.long))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "      print(f'Epoch: {epoch + 1} loss: {running_loss / len(train_loader.dataset):.5f}')\n",
        "      running_loss = 0.0"
      ],
      "metadata": {
        "id": "wDfnXwyLirzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix seeds for good measure\n",
        "seed = 1\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Use the tree dataset\n",
        "dataset = Trees().makedata(num_graphs = 200, num_nodes = 8)\n",
        "\n",
        "model = GIN()\n",
        "# Let's go!\n",
        "train_model(model, dataset)"
      ],
      "metadata": {
        "id": "4iHyKtBmiyIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Until now we used the GIN convolution that is already provided by pytorch geometric. A nice comprehensive overview of all provided convolutions can be found here: [link](https://pytorch-geometric.readthedocs.io/en/latest/notes/cheatsheet.html). Now what if we want to get more advanced and use our own convolution? In the following code snippet a new convolutional layer is defined."
      ],
      "metadata": {
        "id": "TyFqvNDfjGni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import MessagePassing\n",
        "\n",
        "# The secret is to use the MessagePassing class\n",
        "class CustomMP(MessagePassing):\n",
        "    def __init__(self, edge_nn, nn):\n",
        "        super(CustomMP, self).__init__(aggr='add')\n",
        "        self.edge_nn = edge_nn\n",
        "        self.nn = nn\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        return self.nn(x + self.propagate(edge_index, x=x))\n",
        "\n",
        "    # access receiver x_i and sender x_j\n",
        "    def message(self, x_j, x_i):\n",
        "      # more elaborate example to look at both endpoints of an edge\n",
        "      concatted = torch.cat((x_j, x_i), dim=1)\n",
        "      return self.edge_nn(concatted)\n",
        "\n",
        "# This is a GNN as before that uses the new convolutional layer\n",
        "class AdvancedGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = torch.nn.Sequential(torch.nn.Linear(8, 2), torch.nn.ReLU())\n",
        "        self.edge_network = torch.nn.Sequential(torch.nn.Linear(2, 8), torch.nn.ReLU())\n",
        "        self.conv3 = CustomMP(edge_nn = self.edge_network, nn = self.network)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "AYC_K9PKi1Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should now understand the basics of GNNs and are finally ready for your first real challenge using GNNs. For the next task you will only need one more small thing: What if we want to do a prediction for the whole graph and not on a per-node level? In this case we have to do what GNN people call pooling to combine the embeddings of all nodes into one final output. This is usually done by applying a permutation invariant aggregation (sum, mean, max, min, ...) and optionally using a MLP on the aggregate. You will get more information on this in the skeleton of the next task."
      ],
      "metadata": {
        "id": "djomSOuYjLWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chess Prediction Task\n",
        "\n",
        "We are given social networks of novice chess players. The edges consist of a single real $e_{ij} \\in [0,1]$ indicating the playing frequency between player $i$ and player $j$.\n",
        "\n",
        "It has been observed that after some time either the players in the network stop playing chess or the network as a whole matures into a chess community.\n",
        "We know a few factors that are essential to a network to mature into a chess community:\n",
        "- In a chess community there are players considered to be _chess masters_.\n",
        "- _chess master_ players have several friends that are considered _frequent players_.\n",
        "\n",
        "We don't know exactly how many chess masters there are in a chess community nor how to exactly define a frequent player. However we know that it somehow depends on the frequency and the play with other friends (as indicated by the edges in the graph).\n",
        "\n",
        "Given a dataset of such novice chess social networks determine if a network matures into a chess communmity."
      ],
      "metadata": {
        "id": "U_Ne8hQzjNlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! wget -O chess_graphs.pt https://polybox.ethz.ch/index.php/s/R1OJDjSaY5pYXYF/download"
      ],
      "metadata": {
        "id": "OfrTzZ0ujI2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chess_graphs = torch.load('chess_graphs.pt')"
      ],
      "metadata": {
        "id": "80OYYot1jQAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some helper functions to evaluate perfomance"
      ],
      "metadata": {
        "id": "lJIDIeZrjVca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model_comp(model, dataset):\n",
        "  model.eval()\n",
        "  acc = 0\n",
        "  device = torch.device('cpu')\n",
        "  tot_nodes = 0\n",
        "  perf = 0\n",
        "  gpred = []\n",
        "  gsol = []\n",
        "  for step, batch in enumerate(dataset):\n",
        "      tot_nodes += 1\n",
        "      with torch.no_grad():\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch)\n",
        "\n",
        "      y_pred = torch.argmax(pred,dim=1)\n",
        "      graph_acc = torch.sum(y_pred == batch.y).item()\n",
        "      acc += graph_acc\n",
        "\n",
        "  acc = acc\n",
        "  return acc/tot_nodes\n"
      ],
      "metadata": {
        "id": "HSRpNlCYjSYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The starting skeleton for the chess gnn"
      ],
      "metadata": {
        "id": "WC_Lz9w-jZ7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import global_add_pool,global_mean_pool,global_max_pool\n",
        "\n",
        "class Conv1(MessagePassing):\n",
        "  def __init__(self):\n",
        "    # possible aggregations are 'mean', 'max', 'min' or 'add'\n",
        "    # hint: might consider to add an mlp after the propagation\n",
        "    super(Conv1, self).__init__(aggr='TOCHOOSE')\n",
        "\n",
        "  def forward(self, edge_index, edge_attr):\n",
        "    return self.propagate(edge_index, edge_attr=edge_attr)\n",
        "\n",
        "  def message(self, edge_attr):\n",
        "    return edge_attr\n",
        "\n",
        "class Conv2(MessagePassing):\n",
        "  def __init__(self):\n",
        "    super(Conv2, self).__init__(aggr='TOCHOOSE')\n",
        "    # hint: might consider to add an mlp after the propagation\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    return self.propagate(edge_index, x=x)\n",
        "\n",
        "\n",
        "class ChessGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.sample_nn = torch.nn.Sequential(torch.nn.Linear(1, 2), torch.nn.ReLU(), torch.nn.Linear(2, 2), torch.nn.ReLU())\n",
        "        self.conv1 = Conv1()\n",
        "        self.conv2 = Conv2()\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "\n",
        "\n",
        "        # specify the graph convolutions\n",
        "        x = self.conv1(edge_index, edge_attr) # first convolution should consider the edge feature\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        # prediction class is to classify graphs -> need to pool the node information\n",
        "        # possible pooling layers are global_add_pool, global_mean_pool, global_max_pool\n",
        "        # The pooling operation works on a batch of graphs and thus outputs as many elements in the tensor as there are graphs\n",
        "        x = global_add_pool(x, batch)\n",
        "\n",
        "        # x should be logits for 2 classes\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "DCtCbO13jXJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop for the chess gnn"
      ],
      "metadata": {
        "id": "gkhCxwo_jfzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, dataset):\n",
        "  criterion = nn.NLLLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  val_split = 0.8\n",
        "  np.random.shuffle(dataset)\n",
        "  train_size = int(val_split*len(dataset))\n",
        "  train_loader = DataLoader(dataset[:train_size], batch_size=8 , shuffle=True)\n",
        "  val_loader = DataLoader(dataset[train_size:], batch_size=1 , shuffle=True)\n",
        "\n",
        "\n",
        "  model.train()\n",
        "  device = torch.device('cpu')\n",
        "  for epoch in range(100):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "          data = data.to(device)\n",
        "\n",
        "          pred = model(data)\n",
        "          loss = criterion(pred, data.y.to(torch.long)) #TODO: this does not work... \"IndexError: Target 1 is out of bounds.\"\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()*data.num_graphs\n",
        "\n",
        "      print(f'Epoch: {epoch + 1} loss: {running_loss / len(train_loader.dataset):.5f} validation set accuracy: {eval_model_comp(model, val_loader)}')\n",
        "      running_loss = 0.0\n",
        "\n",
        "seed = 1\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "model = ChessGNN()\n",
        "train_model(model, chess_graphs)\n",
        "\n",
        "# with the right architecture you should be able to solve the task in about 50 epochs"
      ],
      "metadata": {
        "id": "2MZWpAFwjeDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLOCK 2: ESCAPE THE MAZE\n"
      ],
      "metadata": {
        "id": "d5TwI_dTjr93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this block you will solve a very practical task, how to find your way in a maze! First, let's look at the concrete task."
      ],
      "metadata": {
        "id": "a0563T8ijuRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def create_dataset(size, num_mazes):\n",
        "  images = []\n",
        "  graphs = []\n",
        "  x = []\n",
        "  y = []\n",
        "  for _ in range(num_mazes):\n",
        "    G = nx.grid_2d_graph(size[0], size[1])\n",
        "    for e in G.edges():\n",
        "      G[e[0]][e[1]]['weight'] = random.uniform(0, 1)\n",
        "    T = nx.minimum_spanning_tree(G)\n",
        "    start, end = random.sample(list(T.nodes), k=2)\n",
        "    path = nx.shortest_path(T, source=start, target=end)\n",
        "\n",
        "    # Add graph\n",
        "    T.colors = ['blue' if n in [start, end] else 'green' for n in T.nodes]\n",
        "    graphs.append(T)\n",
        "\n",
        "    # Create images\n",
        "    image = np.zeros((size[0]*2+1,size[1]*2+1))\n",
        "    image_solution = np.zeros((size[0]*2+1,size[1]*2+1))\n",
        "    for e in T.edges():\n",
        "      image[e[0][0] + e[1][0] + 1][e[0][1] + e[1][1] + 1] = 1\n",
        "      image_solution[e[0][0] + e[1][0] + 1][e[0][1] + e[1][1] + 1] = 1 if not (e[0] in path and e[1] in path) else 2\n",
        "    for n in T.nodes():\n",
        "      image[n[0]*2+1][n[1]*2+1] = 2 if n in [start, end] else 1\n",
        "      image_solution[n[0]*2+1][n[1]*2+1] = 2 if n in [start, end] else (1 if not n in path else 2)\n",
        "    images.append((F.one_hot(torch.tensor(image, dtype=torch.int64)), F.one_hot(torch.tensor(image_solution, dtype=torch.int64))))\n",
        "  return images, graphs\n",
        "\n",
        "import torchvision\n",
        "\n",
        "def show_graph_and_images(graph, images):\n",
        "  fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
        "  nx.draw(graphs[0], pos={node: (node[1], 4-node[0]) for node in graph.nodes}, node_color = graph.colors, ax=ax1)\n",
        "  ax2.imshow(torchvision.transforms.ToPILImage()(images[0].permute(2, 0, 1).float()), interpolation=\"none\")\n",
        "  ax3.imshow(torchvision.transforms.ToPILImage()(images[1].permute(2, 0, 1).float()), interpolation=\"none\")"
      ],
      "metadata": {
        "id": "jlH-QkodjsTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate mazes on a grid of a fixed size and create two different versions of the same dataset. First, we can represent mazes as images. The input for the task are two points (pixels) in the maze that are marked with the same color. We have to find the (unique) path between these two points and mark them in the output. Instead of representing a maze as an image, we can also use graphs for this task. The next cell shows the graph representation of a maze, the according image to the right of it and the ground truth for the image."
      ],
      "metadata": {
        "id": "V3xtH16Pj4JE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, graphs = create_dataset((4, 4), 10)\n",
        "show_graph_and_images(graphs[0], images[0])"
      ],
      "metadata": {
        "id": "6PKP4Bvdj1sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The advantage of using GNNs for this task is that we can apply the same network to graphs of arbitrary sizes. This makes it possible to train on relatively small example graphs and hopefully generalize to much bigger instances. The dataset generation function allows us to generate larger instances too so we can test for that:"
      ],
      "metadata": {
        "id": "UmQz7ytOyUhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images, graphs = create_dataset((8, 8), 10)\n",
        "show_graph_and_images(graphs[0], images[0])"
      ],
      "metadata": {
        "id": "PO6S38XgywDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train your own GNN"
      ],
      "metadata": {
        "id": "95H2ftyV3An9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's your turn, train a graph neural network to escape the maze! The following function creates the dataset containing pyG graphs for you (the previous version only created networkx graphs and images):"
      ],
      "metadata": {
        "id": "glZLpy8jzADU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "def create_dataset_graph(sizes = []):\n",
        "  graphs = []\n",
        "  counternodes = 0\n",
        "  graphIDcounter = 0\n",
        "  for num_mazes, size in sizes:\n",
        "    def h(x):\n",
        "      return x[1]+size[1]*x[0]\n",
        "    for graphID in range(num_mazes):\n",
        "      num_nodes = size[0]*size[1]\n",
        "      G = nx.grid_2d_graph(size[0], size[1])\n",
        "      for e in G.edges():\n",
        "        G[e[0]][e[1]]['weight'] = random.uniform(0, 1)\n",
        "      T = nx.minimum_spanning_tree(G)\n",
        "      start, end = random.sample(list(T.nodes), k=2)\n",
        "      path = nx.shortest_path(T, source=start, target=end)\n",
        "\n",
        "      graph = from_networkx(T)\n",
        "      graph.x = torch.zeros(num_nodes, 2)\n",
        "      graph.x[:,1] = 1\n",
        "      start = h(start)\n",
        "      end = h(end)\n",
        "      graph.x[start] = 1\n",
        "      graph.x[start][1] = 0\n",
        "      graph.x[end][0] = 1\n",
        "      graph.x[end][1] = 0\n",
        "      graph.y = torch.zeros(num_nodes)\n",
        "      graph.ids = torch.zeros(num_nodes)\n",
        "      graph.graphID = graphIDcounter\n",
        "      for node in T.nodes:\n",
        "        graph.ids[h(node)] = counternodes + h(node)\n",
        "      for node in path:\n",
        "        graph.y[h(node)] = 1\n",
        "      graphs.append(graph)\n",
        "      counternodes += num_nodes\n",
        "      graphIDcounter += 1\n",
        "  return graphs"
      ],
      "metadata": {
        "id": "lN6u1tyPkJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_graphs = create_dataset_graph(sizes = [(200,(4,4))])"
      ],
      "metadata": {
        "id": "6I36D2tX3LpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide a function to evaluate the performance of the model. As the distribution of ground truth values tends to be unbalanced (especially for larger graphs), we use the F1 score. Our model will apply the same update rule recurrently. Of course, the number of iterations (and thus node updates) that we need to solve the maze depends on specific instance and size of the graph. To keep things simple, we define a ratio and let the GNN update for `RATIO*n` steps. Feel free to change the ratio if you like to."
      ],
      "metadata": {
        "id": "UEydOH5Qzb5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RATIO = 1"
      ],
      "metadata": {
        "id": "i1OWi25C0xIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import F1Score\n",
        "\n",
        "def eval_model_comp(model, dataset, mode = None):\n",
        "  model.eval()\n",
        "  acc = 0\n",
        "  device = torch.device('cpu')\n",
        "  tot_nodes = 0\n",
        "  tot_graphs = 0\n",
        "  perf = 0\n",
        "  gpred = []\n",
        "  gsol = []\n",
        "  for step, batch in enumerate(dataset):\n",
        "      n = len(batch.x)/batch.num_graphs\n",
        "      with torch.no_grad():\n",
        "        batch = batch.to(device)\n",
        "        pred = model(batch, int(RATIO*n))\n",
        "\n",
        "      if mode == \"small\":\n",
        "        if n > 4*4:\n",
        "          break\n",
        "      elif mode == \"medium\":\n",
        "        if n > 8*8:\n",
        "          break\n",
        "      elif mode == \"large\":\n",
        "        if n > 16*16:\n",
        "          break\n",
        "      y_pred = torch.argmax(pred,dim=1)\n",
        "      graph_acc = torch.sum(y_pred == batch.y).item()\n",
        "      tot_nodes += len(batch.x)\n",
        "      tot_graphs += batch.num_graphs\n",
        "      acc += graph_acc\n",
        "      for p in y_pred:\n",
        "        gpred.append(int(p.item()))\n",
        "      for p in batch.y:\n",
        "        gsol.append(int(p.item()))\n",
        "      if graph_acc == n:\n",
        "        perf += 1\n",
        "  from sklearn.metrics import f1_score\n",
        "  gpred = torch.tensor(gpred)\n",
        "  gsol = torch.tensor(gsol)\n",
        "  f1score = f1_score(gpred, gsol)\n",
        "  acc = acc\n",
        "  return (perf/tot_graphs, acc/tot_nodes, f1score)\n",
        "\n",
        "def eval_model(model, dataset, mode = None):\n",
        "  d = DataLoader(dataset, batch_size=1)\n",
        "  a,b,c = eval_model_comp(model, d, mode = mode)\n",
        "  print(f\"node accuracy: {b:.3f} | node f1 score: {c:.3f} | graph accuracy: {a:.3}\")"
      ],
      "metadata": {
        "id": "dFd43nN_3Oes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maze GNN\n",
        "To make it easier to start, we provide a skeleton for the GNN that you can complete."
      ],
      "metadata": {
        "id": "ghJW3vSX3cPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.nn import MessagePassing\n",
        "\n",
        "class MazeConv(MessagePassing):\n",
        "    def __init__(self):\n",
        "      super(MazeConv, self).__init__(aggr='add')\n",
        "      #TODO\n",
        "      #initialize custom message passing\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "      #TODO\n",
        "      #define own computation, call the round with\n",
        "      #self.propagate(edge_index, x=x)\n",
        "      return x\n",
        "\n",
        "    def message(self, x_j, x_i):\n",
        "      #TODO\n",
        "      #define the custom message that gnns exchange\n",
        "      return x_j\n",
        "\n",
        "\n",
        "class MazeGNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dropout = 0.2\n",
        "\n",
        "        hidden_dim = 8\n",
        "\n",
        "        # we recommend using a larger dimension during the graph computation\n",
        "        # therefore the encoder and decoder map in-/output to the used dimension\n",
        "        self.encoder = self.get_mlp(2,8,hidden_dim)\n",
        "        self.decoder = self.get_mlp(hidden_dim,32,2)\n",
        "\n",
        "        self.conv = MazeConv()\n",
        "\n",
        "    # get the graph and number of nodes in the graph\n",
        "    def forward(self, data, num_nodes):\n",
        "      #things to consider:\n",
        "      # how many convolutions do we need to execute?\n",
        "      # what kind of custom convolution could help\n",
        "      # do we want a skip connection between layers?\n",
        "\n",
        "      x, edge_index = data.x, data.edge_index\n",
        "      input = x\n",
        "\n",
        "      x = self.encoder(x)\n",
        "\n",
        "      #------------------------\n",
        "      # Here you should specify the graph computations\n",
        "\n",
        "      x = self.conv(x, edge_index)\n",
        "\n",
        "      #------------------------\n",
        "\n",
        "      x = self.decoder(x)\n",
        "\n",
        "      #output is logits of belonging to two classes\n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "    # helper function - generates an MLP w. relu activation with 3 layers\n",
        "    def get_mlp(self, input_dim, hidden_dim, output_dim, last_relu = True):\n",
        "        modules = [torch.nn.Linear(input_dim, int(hidden_dim)), torch.nn.ReLU(), torch.nn.Dropout(self.dropout), torch.nn.Linear(int(hidden_dim), output_dim)]\n",
        "        if last_relu:\n",
        "            modules.append(torch.nn.ReLU())\n",
        "        return torch.nn.Sequential(*modules)\n"
      ],
      "metadata": {
        "id": "cP1JHyc93ckV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we provide boilerplate training code as well, feel free to adapt it!"
      ],
      "metadata": {
        "id": "wR2LalKi0DFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_maze_model(model):\n",
        "  criterion = nn.NLLLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "  dataset = training_graphs\n",
        "  val_split = 0.8\n",
        "\n",
        "  np.random.shuffle(dataset)\n",
        "  train_size = int(val_split*len(dataset))\n",
        "  train_loader = DataLoader(dataset[:train_size], batch_size=1, shuffle=True)\n",
        "  val_set = dataset[train_size:]\n",
        "\n",
        "  model.train()\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "\n",
        "  for epoch in range(20):\n",
        "      running_loss = 0.0\n",
        "      for i, data in enumerate(train_loader):\n",
        "          optimizer.zero_grad()\n",
        "          data = data.to(device)\n",
        "\n",
        "\n",
        "          # you can change additional parameters here\n",
        "          pred = model(data, data.num_nodes)\n",
        "\n",
        "          loss = criterion(pred, data.y.to(torch.long))\n",
        "\n",
        "\n",
        "\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "      print(f'Epoch: {epoch + 1} loss: {running_loss / len(train_loader.dataset):.5f}')\n",
        "      print(\"performance on validation set\")\n",
        "      eval_model(model, val_set)\n",
        "      running_loss = 0.0"
      ],
      "metadata": {
        "id": "S-4l_ogl3lpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set seeds for reproducability\n",
        "seed = 10\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "maze_model = MazeGNN()\n",
        "train_maze_model(maze_model)"
      ],
      "metadata": {
        "id": "_YIPLmgz3pvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our aim is to train a model that works well with different graph sizes, we generate our evaluation data as follows:"
      ],
      "metadata": {
        "id": "kaOtMPpc2s4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = create_dataset_graph(sizes = [(200,(4,4)), (200,(8,8)), (200,(12,12))])"
      ],
      "metadata": {
        "id": "56eue6Ia1hXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can test our model. How good can you get?"
      ],
      "metadata": {
        "id": "I7kzGxzN22iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_model(maze_model, eval_dataset)"
      ],
      "metadata": {
        "id": "yMan8zGN1LmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}